[general]
experiment_name=my_experiment
gpu_id=0

[model]
fnet_config=
fnet_checkpoint=

[tokenizer]
# sentencepiece, wordpiece or huggingface
type=sentencepiece
# path to .model file for sentencepiece
# path to vocab file for wordpiece
vocab=
# name of tokenizer (only for huggingface type)
hf_name=

[training]
learning_rate=1e-4
train_batch_size=64
eval_batch_size=64
eval_frequency=500
eval_steps=1000
